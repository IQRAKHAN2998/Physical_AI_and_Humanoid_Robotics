"use strict";(self.webpackChunkphysical_ai_and_humanoid_robotics=self.webpackChunkphysical_ai_and_humanoid_robotics||[]).push([[715],{8453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>r});var o=n(6540);const i={},s=o.createContext(i);function a(e){const t=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(s.Provider,{value:t},e.children)}},9848:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4-vla/capstone-project","title":"Capstone Project","description":"Capstone Project: The Autonomous Humanoid","source":"@site/docs/module-4-vla/04-capstone-project.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/capstone-project","permalink":"/docs/module-4-vla/capstone-project","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Capstone Project"},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning","permalink":"/docs/module-4-vla/cognitive-planning"},"next":{"title":"Why Physical AI Matters","permalink":"/docs/why-physical-ai-matters"}}');var i=n(4848),s=n(8453);const a={sidebar_position:4,title:"Capstone Project"},r=void 0,c={},l=[{value:"Capstone Project: The Autonomous Humanoid",id:"capstone-project-the-autonomous-humanoid",level:3}];function d(e){const t={h3:"h3",p:"p",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h3,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"}),"\n",(0,i.jsx)(t.p,{children:"The Capstone Project is designed to synthesize all the skills learned throughout the course, challenging students to build a fully autonomous humanoid robot in a simulated environment. This project integrates voice recognition, cognitive planning, perception, navigation, and manipulation into a single cohesive system."}),"\n",(0,i.jsx)(t.p,{children:'Students program a humanoid robot to respond to natural language commands using the Voice-to-Action pipeline. For example, the instruction "Pick up the red cube from the table" triggers the robot to plan its path, avoid obstacles, and approach the target. The robot uses its simulated sensors\u2014including cameras, LiDAR, and depth sensors\u2014to identify objects, understand the environment, and make real-time decisions.'}),"\n",(0,i.jsx)(t.p,{children:"Cognitive Planning and LLM integration allow the robot to break down complex tasks into sequential actions, while ROS 2 nodes control motion, perception, and manipulation. Students must also ensure smooth interaction between AI perception modules, navigation stacks like Nav2, and manipulation controllers, handling unexpected situations such as obstacles or missing objects."}),"\n",(0,i.jsx)(t.p,{children:"By completing this Capstone Project, learners demonstrate mastery of Vision-Language-Action concepts, sensor integration, AI-driven decision-making, and humanoid robotics control. The project provides a realistic, hands-on experience, preparing students to design autonomous robots capable of intelligent interaction and task execution in both simulated and real-world environments."})]})}function u(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);