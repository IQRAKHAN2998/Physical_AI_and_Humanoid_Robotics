"use strict";(self.webpackChunkphysical_ai_and_humanoid_robotics=self.webpackChunkphysical_ai_and_humanoid_robotics||[]).push([[636],{5360:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/overview","title":"Overview","description":"Module 4: Vision-Language-Action (VLA)","source":"@site/docs/module-4-vla/01-overview.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/overview","permalink":"/docs/module-4-vla/overview","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Overview"},"sidebar":"tutorialSidebar","previous":{"title":"Nav2","permalink":"/docs/module-3-ai-robot-brain/nav2"},"next":{"title":"Voice-to-Action","permalink":"/docs/module-4-vla/voice-to-action"}}');var i=o(4848),a=o(8453);const s={sidebar_position:1,title:"Overview"},r=void 0,l={},c=[{value:"Module 4: Vision-Language-Action (VLA)",id:"module-4-vision-language-action-vla",level:3}];function d(e){const n={h3:"h3",p:"p",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h3,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"}),"\n",(0,i.jsx)(n.p,{children:"Focus: The convergence of LLMs and Robotics."}),"\n",(0,i.jsx)(n.p,{children:"Module 4 explores the integration of advanced AI models, particularly Large Language Models (LLMs), with robotic systems to enable Vision-Language-Action (VLA) capabilities. This module focuses on how humanoid robots can perceive their environment, understand natural language instructions, and perform corresponding actions autonomously."}),"\n",(0,i.jsx)(n.p,{children:"Students learn to combine visual perception from cameras and depth sensors with natural language understanding, allowing robots to interpret commands, recognize objects, and plan tasks accordingly. The module covers cognitive planning, voice-to-action systems, and multimodal learning, illustrating how AI can bridge sensory input and decision-making for complex human-robot interactions."}),"\n",(0,i.jsx)(n.p,{children:"Hands-on exercises include programming a humanoid robot to respond to verbal instructions, identify objects in its surroundings, and execute contextually appropriate actions. By integrating LLMs with robotics control frameworks, students see how natural language processing can drive autonomous behavior, enabling robots to perform tasks that require reasoning and adaptability."}),"\n",(0,i.jsx)(n.p,{children:"By the end of this module, learners will understand the principles of VLA, how to fuse vision and language with robot control, and the practical challenges of implementing AI-driven humanoid actions. This knowledge equips students to design robots capable of intelligent interaction in real-world environments, paving the way for more sophisticated AI applications in robotics."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>r});var t=o(6540);const i={},a=t.createContext(i);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);