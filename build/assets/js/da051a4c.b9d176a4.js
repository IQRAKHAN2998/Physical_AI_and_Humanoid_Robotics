"use strict";(self.webpackChunkphysical_ai_and_humanoid_robotics=self.webpackChunkphysical_ai_and_humanoid_robotics||[]).push([[970],{454:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>u,frontMatter:()=>c,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4-vla/voice-to-action","title":"Voice-to-Action","description":"Voice-to-Action","source":"@site/docs/module-4-vla/02-voice-to-action.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/voice-to-action","permalink":"/docs/module-4-vla/voice-to-action","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Voice-to-Action"},"sidebar":"tutorialSidebar","previous":{"title":"Overview","permalink":"/docs/module-4-vla/overview"},"next":{"title":"Cognitive Planning","permalink":"/docs/module-4-vla/cognitive-planning"}}');var i=n(4848),a=n(8453);const c={sidebar_position:2,title:"Voice-to-Action"},r=void 0,s={},l=[{value:"Voice-to-Action",id:"voice-to-action",level:3}];function d(e){const t={h3:"h3",p:"p",strong:"strong",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h3,{id:"voice-to-action",children:"Voice-to-Action"}),"\n",(0,i.jsxs)(t.p,{children:["The Voice-to-Action module introduces students to the integration of speech recognition with humanoid robotics, enabling robots to understand and execute commands given through natural language. Leveraging tools like ",(0,i.jsx)(t.strong,{children:"OpenAI Whisper"}),", students learn to convert voice input into actionable instructions that a robot can process and perform."]}),"\n",(0,i.jsx)(t.p,{children:"In this module, learners explore the architecture for mapping spoken language to robotic actions. This involves capturing audio input, transcribing it into text, interpreting intent, and linking it with robot control commands. For example, a student can program a humanoid robot to pick up an object, navigate to a location, or perform a sequence of tasks based on verbal instructions."}),"\n",(0,i.jsx)(t.p,{children:"Practical exercises include creating robust pipelines that handle noise, accents, and ambiguous commands, ensuring reliable voice-based interaction. Students also learn to integrate the voice interface with ROS 2 and AI modules, enabling the robot to react in real time to dynamic environments."}),"\n",(0,i.jsx)(t.p,{children:"By the end of this module, students gain hands-on experience in designing voice-driven humanoid robots, understanding both the technical challenges and practical solutions. Mastery of Voice-to-Action equips learners to build robots capable of natural and intuitive interaction, laying the foundation for advanced Vision-Language-Action applications."})]})}function u(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>c,x:()=>r});var o=n(6540);const i={},a=o.createContext(i);function c(e){const t=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:c(e.components),o.createElement(a.Provider,{value:t},e.children)}}}]);